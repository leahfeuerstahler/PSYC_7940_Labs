{
  "hash": "ed51e378a9e1a399e373f81b921e9d29",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 7: Markov Chain Monte Carlo\"\ndate: March 4, 2026\n---\n\n## Review\n\n-   In previous labs, we conducted Bayesian analysis for regression models. We also learned how to summarize the results.\n\n-   However, we never investigated the quality of analysis itself. Today we want to answer the question: whether the results accurately describe the posterior distribution or not?\n\n## Markov Chain Monte Carlo (MCMC)\n\n-   If you remember, we mentioned the word **MCMC** several times. The full name of it is Markov Chain Monte Carlo, and it is the method for us to generate samples from a posterior distribution. \n\n-   In this lab, we focus on Hamiltonian Monte Carlo (or Hybrid Monte Carlo), which is a specific type of MCMC. \n\n## Markov Chain Monte Carlo (MCMC)\n\n**What to pay attention to?**\n\n-   Choices when running the model:\n    -   Iterations\n    -   Chains\n\n-   Ways to analyze the output:\n    -   Traceplot\n    -   Effective sample size\n    -   Gelman's R\n    -   Pairs plots\n    \n-   General tips for error message.\n    \n## brms Model Mechanics\n\nRead in the TDF data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\nTDF <- read_csv(\"TDF.csv\")\nTDF<-TDF %>% \n  # mean-center height and convert to cm\n  mutate (HeightC = (Height - mean(Height))*100)\n```\n:::\n\n\n\nFit the model:\n\n::: {.cell}\n\n```{.r .cell-code}\nbrms_form <- Weight ~ HeightC\n\nbrms_prior <- prior(normal(66.5, 2), class = \"Intercept\") +\n              prior(normal(1, .5), class = \"b\") +\n              prior(exponential(.25), class = \"sigma\")\n\nmod1<-brm(brms_form,\n          data=TDF,\n          family = gaussian(link = \"identity\"),\n          prior = brms_prior,\n          seed = 123)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 17.0.0 (clang-1700.6.3.2)’\nusing SDK: ‘MacOSX26.2.sdk’\nclang -arch arm64 -std=gnu2x -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n  679 | #include <cmath>\n      |          ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 1:                0.007 seconds (Sampling)\nChain 1:                0.017 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 2:                0.007 seconds (Sampling)\nChain 2:                0.017 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 3:                0.007 seconds (Sampling)\nChain 3:                0.016 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 4:                0.007 seconds (Sampling)\nChain 4:                0.017 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n## brms Model Mechanics\n\nIf we scroll through the messages, we can see that it provides informations about **chains** and **iterations**.\n\n```{}\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 1:                0.041 seconds (Sampling)\nChain 1:                0.053 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 2:                0.009 seconds (Sampling)\nChain 2:                0.022 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 3:                0.007 seconds (Sampling)\nChain 3:                0.02 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 4:                0.008 seconds (Sampling)\nChain 4:                0.02 seconds (Total)\nChain 4: \n```\n\n\n## brms Model Mechanics\n\nUp until now, we used the default settings of `brm`:\n\n* **4 chains**: run 4 separate MCMC chains.\n\n* **2000 iterations per chain** 2000 samples are taken from each of chains.\n\n* Chains are run linearly, one after the other.\n\n::: {.callout-note}\n\nNote that $2,000\\times 4 = 8,000$ samples are run, but only 4,000 samples contribute to the posterior estimates. \n\nThis is because the HMC needs a warm-up process to make the sampling process more efficient. After the warm-up process, the `brm` starts the real sampling process. By default, half of the iterations in each chain are used for warm-up.\n\n:::\n\n\n## brms Model Mechanics\n\nWe can modify the following argument:\n\n* `chains`: number of chains to run\n\n* `iter`: number of iterations per chain\n\n* `warmup`: number of warm-up iterations (less than `iter`)\n\n* `cores`: number of computer cores for running chains in parallel (should be no larger than `chains`)\n\n::: {.callout-tip}\n\nRun the following to see the maximum number of cores available on your computer. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nparallel::detectCores()\n```\n:::\n\n\n:::\n\n## brms Model Mechanics\n\nModify the `chains`, `iter`, and `cores` arguments as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod2 <- brm(brms_form,\n            data = TDF,\n            family = gaussian(link = \"identity\"),\n            prior = brms_prior,\n            chains = 2,\n            iter = 1000,\n            warmup = 300,\n            cores = 2,\n            seed = 123)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 17.0.0 (clang-1700.6.3.2)’\nusing SDK: ‘MacOSX26.2.sdk’\nclang -arch arm64 -std=gnu2x -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n  679 | #include <cmath>\n      |          ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n```\n\n\n:::\n:::\n\n\n## brms Model Mechanics\n\nFor illustration, let's also run the model with relatively few iterations and warmup, and with **very bad** priors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbad_priors <- prior(normal(0, 1), class = \"Intercept\") +\n              prior(normal(-20, .1), class = \"b\") +\n              prior(exponential(100), class = \"sigma\")\n\nmod3 <- brm(brms_form,\n            data = TDF,\n            family = gaussian(link = \"identity\"),\n            prior = bad_priors,\n            chains = 4,\n            iter = 100,\n            warmup = 50,\n            seed = 123)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 17.0.0 (clang-1700.6.3.2)’\nusing SDK: ‘MacOSX26.2.sdk’\nclang -arch arm64 -std=gnu2x -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n  679 | #include <cmath>\n      |          ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: WARNING: There aren't enough warmup iterations to fit the\nChain 1:          three stages of adaptation as currently configured.\nChain 1:          Reducing each adaptation stage to 15%/75%/10% of\nChain 1:          the given number of warmup iterations:\nChain 1:            init_buffer = 7\nChain 1:            adapt_window = 38\nChain 1:            term_buffer = 5\nChain 1: \nChain 1: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 1: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 1: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 1: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 1: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 1: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 1: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 1: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 1: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 1: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 1: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 1: Iteration: 100 / 100 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.006 seconds (Warm-up)\nChain 1:                0.003 seconds (Sampling)\nChain 1:                0.009 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: WARNING: There aren't enough warmup iterations to fit the\nChain 2:          three stages of adaptation as currently configured.\nChain 2:          Reducing each adaptation stage to 15%/75%/10% of\nChain 2:          the given number of warmup iterations:\nChain 2:            init_buffer = 7\nChain 2:            adapt_window = 38\nChain 2:            term_buffer = 5\nChain 2: \nChain 2: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 2: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 2: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 2: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 2: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 2: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 2: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 2: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 2: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 2: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 2: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 2: Iteration: 100 / 100 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.004 seconds (Warm-up)\nChain 2:                0.005 seconds (Sampling)\nChain 2:                0.009 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: WARNING: There aren't enough warmup iterations to fit the\nChain 3:          three stages of adaptation as currently configured.\nChain 3:          Reducing each adaptation stage to 15%/75%/10% of\nChain 3:          the given number of warmup iterations:\nChain 3:            init_buffer = 7\nChain 3:            adapt_window = 38\nChain 3:            term_buffer = 5\nChain 3: \nChain 3: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 3: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 3: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 3: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 3: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 3: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 3: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 3: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 3: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 3: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 3: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 3: Iteration: 100 / 100 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.002 seconds (Warm-up)\nChain 3:                0.004 seconds (Sampling)\nChain 3:                0.006 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: WARNING: There aren't enough warmup iterations to fit the\nChain 4:          three stages of adaptation as currently configured.\nChain 4:          Reducing each adaptation stage to 15%/75%/10% of\nChain 4:          the given number of warmup iterations:\nChain 4:            init_buffer = 7\nChain 4:            adapt_window = 38\nChain 4:            term_buffer = 5\nChain 4: \nChain 4: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 4: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 4: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 4: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 4: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 4: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 4: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 4: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 4: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 4: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 4: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 4: Iteration: 100 / 100 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.003 seconds (Warm-up)\nChain 4:                0.004 seconds (Sampling)\nChain 4:                0.007 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n\n## brms Model Mechanics\n\nWe can compare the result of `mod1`, `mod2`, and `mod3`. What differences in parameter estimates and CIs do you notice?\n\n::: panel-tabset\n\n## Model 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity \nFormula: Weight ~ HeightC \n   Data: TDF (Number of observations: 179) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    69.31      0.36    68.64    70.01 1.00     3803     2786\nHeightC       0.79      0.05     0.68     0.89 1.00     4017     2589\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.85      0.26     4.39     5.38 1.00     3755     3089\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Model 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity \nFormula: Weight ~ HeightC \n   Data: TDF (Number of observations: 179) \n  Draws: 2 chains, each with iter = 1000; warmup = 300; thin = 1;\n         total post-warmup draws = 1400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    69.29      0.35    68.61    69.97 1.00     1163      909\nHeightC       0.78      0.05     0.68     0.89 1.01     1520      912\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.84      0.26     4.36     5.36 1.00     1427     1185\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Model 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity \nFormula: Weight ~ HeightC \n   Data: TDF (Number of observations: 179) \n  Draws: 4 chains, each with iter = 100; warmup = 50; thin = 1;\n         total post-warmup draws = 200\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     9.70      2.02     2.16    12.09 1.21       16       13\nHeightC     -18.55      0.10   -18.78   -18.36 1.07       55       44\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    32.49      0.41    31.75    33.45 1.08       37       19\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## brms Model Mechanics\n\nThere is little difference in the estimates for Models 1 and 2, but big differences for Model 3. \n\n* Especially for simple models, it may not be necessary to run overly many chains or iterations. There are no golden rules for the selection of chains and iterations, though sometimes our diagnostics suggest that more are needed.\n\nGeneral guidance:\n\n-   Use one chain for testing code. (Better for seeing error messages.)\n\n-   Use three or four chains to run the models. (Allows for comparison among chains)\n\n-   Run chains in parallel only if you have the computational resources to do so. If your computer is slow, running chains in parallel may overwhelm your system.\n\n\n## Diagnostics\n\nWe always want to check that the MCMC algorithm accurately sampled from the posterior distribution. \n\nDensity and trace plots are good places to start. Density plots (left) show the posterior distribution of each parameter. Trace plots (right) show the parameter estimate across iterations, one line per chain. Chains that \"mix well\" will have a lot of overlap.\n\n## Diagnostics\n\n::: panel-tabset\n\n## Model 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mod1)\n```\n\n::: {.cell-output-display}\n![](Markov_Chain_Monte_Carlo_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n## Model 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mod2)\n```\n\n::: {.cell-output-display}\n![](Markov_Chain_Monte_Carlo_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n## Model 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mod3)\n```\n\n::: {.cell-output-display}\n![](Markov_Chain_Monte_Carlo_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n## Diagnostics\n\n\nIt is somewhat subjective to decide whether a trace plot is a \"good\" or not. \n\n* Usually, we want the trace plot to stay in the same $y$ axis range from the beginning to the end, and to explore the full range many times throughout the trajectory.\n\n* Also, we want the trace plot from different chains to explore the same range.\n\n* In the examples on the previous slide, Models 1 and 2 are good, but Model 3's trace plots indicate that there is something wrong with the model.\n\n## Diagnostics\n\nIt is also useful to look at the \"pairs\" plot that shows the relationships between parameter estimates. If there are strong relationships between parameters, it could be the result of multicollinearity or similar problems with your model.\n\n::: panel-tabset\n\n## Model 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(mod1)\n```\n\n::: {.cell-output-display}\n![](Markov_Chain_Monte_Carlo_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n## Model 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(mod2)\n```\n\n::: {.cell-output-display}\n![](Markov_Chain_Monte_Carlo_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n## Model 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(mod3)\n```\n\n::: {.cell-output-display}\n![](Markov_Chain_Monte_Carlo_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n\n## Diagnostics\n\nThe Rhat diagnostic compares the variance in estimates between vs. across chains. Lower (closer to 1) values are better. There are no universal guidelines, but I usually worry if this is greater then 1.01. \n\n* This is printed by `summary()`, but we can also call `rhat()` directly.\n\n* Again, we see that this diagnotic flags problems with Model 3. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrhat(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nb_Intercept   b_HeightC       sigma   Intercept      lprior        lp__ \n   1.000732    1.000409    1.000658    1.000734    1.001033    1.000163 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrhat(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nb_Intercept   b_HeightC       sigma   Intercept      lprior        lp__ \n  0.9996243   1.0149934   1.0025389   0.9996243   0.9992806   1.0149274 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrhat(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nb_Intercept   b_HeightC       sigma   Intercept      lprior        lp__ \n   1.212311    1.073329    1.078070    1.212311    1.019451    1.209169 \n```\n\n\n:::\n:::\n\n\n\n## Diagnostics\n\nAnother useful index is the effective sample size (ESS). This represents the number of effectively independent draws from the posterior distribution. The more iterations you run, the higher this number will *usually* be. \n\n* `Bulk_ESS` is the ESS for the mean or median - high values desired for point estimates\n\n* `Tail_ESS` is for the tails - high values desired for interval estimates\n\n## Diagnostics\n\nCompare the ESS for our 3 model, keeping in mind the \"total post-warmup draws\":\n\n::: panel-tabset\n\n## Model 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity \nFormula: Weight ~ HeightC \n   Data: TDF (Number of observations: 179) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    69.31      0.36    68.64    70.01 1.00     3803     2786\nHeightC       0.79      0.05     0.68     0.89 1.00     4017     2589\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.85      0.26     4.39     5.38 1.00     3755     3089\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Model 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity \nFormula: Weight ~ HeightC \n   Data: TDF (Number of observations: 179) \n  Draws: 2 chains, each with iter = 1000; warmup = 300; thin = 1;\n         total post-warmup draws = 1400\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    69.29      0.35    68.61    69.97 1.00     1163      909\nHeightC       0.78      0.05     0.68     0.89 1.01     1520      912\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.84      0.26     4.36     5.36 1.00     1427     1185\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Model 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity \nFormula: Weight ~ HeightC \n   Data: TDF (Number of observations: 179) \n  Draws: 4 chains, each with iter = 100; warmup = 50; thin = 1;\n         total post-warmup draws = 200\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     9.70      2.02     2.16    12.09 1.21       16       13\nHeightC     -18.55      0.10   -18.78   -18.36 1.07       55       44\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    32.49      0.41    31.75    33.45 1.08       37       19\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n:::\n\n## Tips\n\nIf we encounter an issue during the analysis, it is an indicator that **something** is wrong with our model. Although the best solution is not always clear, we can start with some general tips:\n\nWarning messages:\n\n* It is important to read the content of warning message because it may give essential message why we have a problematic result. Warning messages often suggest useful help pages!\n\n* For example, if the warning message informs of convergence issue (low R_hat or low ESS), often we can first try to increase the number of iterations.\n\n* Sometimes you can ignore the warning message after checking the output. For example, if the R_hat and ESS in the output is good enough to you, having only a few divergent transition may be not troublesome.\n\n\n## Tips\n\nSuggestions if you run into other problems with `brms`:\n\n* Your code may have errors - look over it carefully.\n\n* Make sure that your variables are scaled as you desire and that the priors are appropriate for the scale of your variables.\n\n* Load the relevant data set and package, make sure the name of the object and variables is what you use in the `brm()` function.\n\n* No errors or typos in your models (possible if you have a complicated model)!\n\n* Try generating data that fit your expected model. Do the simulated data produce the same problems? This strategy can help you diagnose whether the problem is with your data or with the model.\n\n## Activity 1\n\nRun the following code that generates final exam grades for 50 students. Grades of 60 or above pass. What happens if we try to predict `pass` from `grade`?\n\n* We will use logistic regression for this analysis\n\n* This example induces **perfect separation**, where the outcome is perfectly predicted. If we try to run this with frequentist methods, we receive errors. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nn <- 50\ngrade <- sample(n, 50, 100)\n\npass <- ifelse(grade > 60, 1, 0) \n\nd <- data.frame(grade, pass)\n```\n:::\n\n\n## Activity 1\n\nUse the following code to run the model with the `brms` default priors. What do you see in the output that indicates problems?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- brm(pass ~ grade, data = d, \n  family = bernoulli(link = \"logit\"),\n  chains = 2, iter = 2000)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit1)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit1)\n```\n:::\n\n\n## Activity 1\n\nUnlike frequentist statistics, Bayesian statistics is capable of giving us a solution to this model. The solution is to add an informative prior.\n\n* Run the following code and compare the results to the previous analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior <- prior(normal(0, 1), class = \"Intercept\") + \n         prior(normal(0, 1), class = \"b\")\n\nfit2 <- brm(pass ~ grade, data = d, \n  family = bernoulli(link = \"logit\"),\n  chains = 2, iter = 2000, prior = prior)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit2)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit2)\n```\n:::\n\n\n\n## Activity 2\n\nNext, we will analyze the `rock` data from the `datasets` package. We will predict permeability `perm` from the interaction of area (`area`) and perimeter (`peri`).\n\n* Use `summary` to look at the ranges of the variables. \n\n* We will use the default uninformative priors for illustration only.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(rock, package = \"datasets\")\n\nsummary(rock)\n\nbrm_form <- perm ~ area * peri\n\nfit <- brm(brm_form, rock,\n           family = gaussian(link = \"identity\"))\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(fit)\n```\n:::\n\n\n\n## Activity 2\n\nRescaling variables can help stabilize the models. Here, we will use the `scale` function to turn each variable into $z$-scores. \n\n* Run the following code (again with uninformative default priors) and compare the results to the previous analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrock2 <- rock %>% mutate(across(everything(), scale))\n\nfit2 <- brm(brm_form, rock2,\n            family = gaussian(link = \"identity\"))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit2)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(fit2)\n```\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}