{
  "hash": "e9347dc2a0b8f0077a3602a9b163b479",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 5: Regression for Causal Inference I\"\ndate: February 18, 2026\n---\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Overview\n\nToday, we will learn how to conduct multiple linear regression using `brms`.\n\n* Prior prediction and specification with multiple variables\n\n* Fiting the models and analyzing results\n\nLoad in the following packages (install if needed):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayesplot)\n```\n:::\n\n\n## Relationships among Multiple Variables\n\n* Last week, we learned how to do linear regression with one predictor.\n\n* This week, we will extend linear regression to have multiple predictors.\n\n* Unlike simple linear regression, we now need to consider relationships among multiple variables.\n\n## Emerging Adulthood\n\nIn lecture, we considered the following model:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Regression_for_Causal_Inference_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\nIf the mediation model is correct then if we include both `soc_med` and `support` as predictors of `wb`, then the coefficient for `soc_med` should be zero.\n\n## Emerging Adulthood\n\nThe model we will fit first:\n\n\\begin{align*}\n  {\\sf wb}_i & \\sim N(\\mu_i, \\sigma) \\\\\n  \\mu_i & = a + bsp \\times {\\sf support}_i + bsm \\times {\\sf soc\\_med}_i \\\\\n  a & \\sim \\_\\_\\_ \\\\\n  bsp & \\sim \\_\\_\\_ \\\\\n  bsm & \\sim \\_\\_\\_ \\\\\n  \\sigma & \\sim \\_\\_\\_ \\\\\n\\end{align*}\n\nLoad in the Emerging Adulthood data from Blackboard:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- read_csv(\"emerging_adult.csv\")\n```\n:::\n\n\n\n## Rescaling Variables\n\nRescaling variables makes prior prediction and model interpretation easier.\n\nLet's look at the scale of our original variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd1 <- d %>% select(soc_med, support, wb)\nd1 %>% summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    soc_med         support            wb       \n Min.   : 0.00   Min.   : 0.00   Min.   :1.000  \n 1st Qu.:29.00   1st Qu.:59.00   1st Qu.:3.500  \n Median :35.00   Median :69.00   Median :4.667  \n Mean   :34.43   Mean   :66.33   Mean   :4.470  \n 3rd Qu.:40.00   3rd Qu.:76.00   3rd Qu.:5.500  \n Max.   :55.00   Max.   :84.00   Max.   :7.000  \n NA's   :1       NA's   :1       NA's   :2      \n```\n\n\n:::\n:::\n\n* `soc_med` is between 0 and 55\n\n* `support` is between 0 and 84\n\n* `wb` is between 1 and 7\n\n## Rescaling Variables\n\nIn lecture, we rescaled these variables to range between 0 and 1.\n\nTo broaden our skills, today we will transform our variables into **$z$-scores** with mean 0 and standard deviation 1. \n\nWe will also retain only complete cases. We will learn about missing data handling next week.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd1 <- d1 %>% filter(complete.cases(d1)) %>%\n  mutate(soc_med = (soc_med - mean(soc_med)) / sd(soc_med),\n         support = (support - mean(support)) / sd(support),\n         wb = (wb - mean(wb)) / sd(wb))\n```\n:::\n\n\n\n## Prior Prediction\n\nRelevant ideas for choosing priors (some borrowed from lecture):\n\n* Data are messy, we want to allow relatively large $\\sigma$ considering the scale of the variables\n\n* Want to allow both positive and negative relationships\n\n* Relationships may not be very strong\n\n* All variables have a mean 0 and standard deviation 1, range from $\\approx$ -3 to +3\n\n## Prior Prediction\n\nWe may try the following priors:\n\n$a \\sim N(0, .1)$\n\n  - We may expect the intercept to be very close to zero\n\n  - Recall that the intercept *must* equal zero for OLS regression\n  \n$bsp, bsm \\sim N(0, .5)$\n\n  - Allow both positive and negative slopes but without expecting very strong slopes\n\n$\\sigma \\sim Exp(1)$\n\n  - This gives $E(\\sigma) = 1$, so that on average, our predictions may be 1 unit off\n\n## Prior Prediction\n\nView the prior structure:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(wb ~ soc_med + support, d1, family = gaussian(link = \"identity\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` nowrap-code\n                  prior     class    coef group resp dpar nlpar lb ub tag\n                 (flat)         b                                        \n                 (flat)         b soc_med                                \n                 (flat)         b support                                \n student_t(3, 0.1, 2.5) Intercept                                        \n   student_t(3, 0, 2.5)     sigma                                0       \n       source\n      default\n (vectorized)\n (vectorized)\n      default\n      default\n```\n\n\n:::\n:::\n\n\n## Prior Prediction\n\nSample from the prior:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior1 <- prior(normal(0, .1), class = \"Intercept\") + \n          prior(normal(0, .5), class = \"b\", coef = \"soc_med\") + \n          prior(normal(0, .5), class = \"b\", coef = \"support\") + \n          prior(exponential(1), class = \"sigma\")\n\nprior_check1 <- brm(wb ~ soc_med + support, d1, prior = prior1,\n        family = gaussian(link = \"identity\"), sample_prior = \"only\")\n```\n:::\n\n\n\n## Prior Prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(conditional_effects(prior_check1, effect = \"soc_med\",\n                         method = \"posterior_linpred\",\n                         spaghetti = TRUE, ndraws = 100))\n```\n\n::: {.cell-output-display}\n![](Regression_for_Causal_Inference_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n## Prior Prediction\n\nNow set `effect = \"...\"` to plot a specific predictor.\n\n::: panel-tabset\n\n## Predictor: `soc_med`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(conditional_effects(prior_check1, effect = \"soc_med\", \n                         method = \"posterior_linpred\",\n                         spaghetti = TRUE, ndraws = 100))\n```\n\n::: {.cell-output-display}\n![](Regression_for_Causal_Inference_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n## Predictor: `support`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(conditional_effects(prior_check1, effect = \"support\", \n                         method = \"posterior_linpred\",\n                         spaghetti = TRUE, ndraws = 100))\n```\n\n::: {.cell-output-display}\n![](Regression_for_Causal_Inference_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n## Prior Prediction\n\nNow set `effect = \"...\"` to plot a specific predictor.\n\n::: panel-tabset\n\n## Predictor: `soc_med`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(conditional_effects(prior_check1, effect = \"soc_med\", \n                         method = \"posterior_predict\"))\n```\n\n::: {.cell-output-display}\n![](Regression_for_Causal_Inference_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n## Predictor: `support`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(conditional_effects(prior_check1, effect = \"support\", \n                         method = \"posterior_predict\"))\n```\n\n::: {.cell-output-display}\n![](Regression_for_Causal_Inference_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Prior Prediction\n\nWe can also look at the predicted distribution of errors (observed - predicted outcomes, a function of $\\sigma$) using `pp_check`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(prior_check1, type = \"error_hist\", ndraws = 12)\n```\n\n::: {.cell-output-display}\n![](Regression_for_Causal_Inference_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n## Fit the Model\n\nAt this point, we will move forward with these priors. As always, reasonable people may choose different priors and you may consider tweaking these choices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 <- brm(wb ~ soc_med + support, d1, prior = prior1,\n          family = gaussian(link = \"identity\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.047 seconds (Warm-up)\nChain 1:                0.054 seconds (Sampling)\nChain 1:                0.101 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 8e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.047 seconds (Warm-up)\nChain 2:                0.046 seconds (Sampling)\nChain 2:                0.093 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 7e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.046 seconds (Warm-up)\nChain 3:                0.049 seconds (Sampling)\nChain 3:                0.095 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 8e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.046 seconds (Warm-up)\nChain 4:                0.052 seconds (Sampling)\nChain 4:                0.098 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n## Summarize Model Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity \nFormula: wb ~ soc_med + support \n   Data: d1 (Number of observations: 3131) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.00      0.02    -0.03     0.03 1.00     4464     2678\nsoc_med       0.01      0.02    -0.02     0.04 1.00     4136     2977\nsupport       0.47      0.02     0.43     0.50 1.00     4138     3168\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.88      0.01     0.86     0.91 1.00     4174     2716\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Counterfactuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(conditional_effects(m1, effect = \"soc_med\", method = \"posterior_linpred\",\n                         conditions = list(support = 0)))\n```\n\n::: {.cell-output-display}\n![](Regression_for_Causal_Inference_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n## Counterfactuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(conditional_effects(m1, effect = \"soc_med\", method = \"posterior_predict\",\n                         conditions = list(support = 0)), points = TRUE)\n```\n\n::: {.cell-output-display}\n![](Regression_for_Causal_Inference_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n## Mediation Models\n\nTo fully test our DAG, we should fit a mediation model, which model modifies the previous model by simultaneously estimating the effect of `soc_med` on `support`. \n\n::: columns\n:::: column\n\n\\begin{align*}\n  {\\sf wb}_i & \\sim N(\\mu_{1i}, \\sigma_1) \\\\\n  \\mu_{1i} & = a_1 + bsp \\times {\\sf support}_i + \\\\\n  & bsm_1 \\times {\\sf soc\\_med}_i \\\\\n  a_1 & \\sim N(0, .1) \\\\\n  bsp & \\sim N(0, .5) \\\\\n  bsm_1 & \\sim N(0, .5) \\\\\n  \\sigma_1 & \\sim Exp(1)\n\\end{align*}\n\n::::\n:::: column\n\n\\begin{align*}\n  {\\sf support}_i & \\sim N(\\mu_{2i}, \\sigma_2) \\\\\n  \\mu_{2i} & = a_2 + bsm_2 \\times {\\sf soc\\_med}_i \\\\\n  a_2 & \\sim N(0, .1) \\\\\n  bsm_2 & \\sim N(0, .5) \\\\\n  \\sigma_2 & \\sim Exp(1) \\\\\n\\end{align*}\n\n::::\n:::\n\nFor simplicity, we will use the same priors as for the multiple regression model, but we could do more prior predictive simulation if desired.\n\n## Mediation Models\n\nWhen testing a mediated model, we use multiple equations to define the relationship. In `brms`, we can do this using the `bf` function.\n\n* Put each regression within `bf()`, separated by `+`\n\n* `set_rescor(FALSE)` means to **not** estimate the correlation among residuals, which we don't want for this model.\n\nModel formula:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_mod_2 <- bf(wb ~ support + soc_med) + \n             bf(support ~ soc_med) + \n             set_rescor(FALSE)\n```\n:::\n\n\n## Mediation Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(brm_mod_2, data = d1, family = gaussian(link = \"identity\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` nowrap-code\n                  prior     class    coef group    resp dpar nlpar lb ub tag\n                 (flat)         b               support                     \n                 (flat)         b soc_med       support                     \n student_t(3, 0.2, 2.5) Intercept               support                     \n   student_t(3, 0, 2.5)     sigma               support             0       \n                 (flat)         b                    wb                     \n                 (flat)         b soc_med            wb                     \n                 (flat)         b support            wb                     \n student_t(3, 0.1, 2.5) Intercept                    wb                     \n   student_t(3, 0, 2.5)     sigma                    wb             0       \n       source\n      default\n (vectorized)\n      default\n      default\n      default\n (vectorized)\n (vectorized)\n      default\n      default\n```\n\n\n:::\n:::\n\n\n## Mediation Models\n\nIn this case, we must specify priors for each response variable separately using the resp argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_prior_2 <- prior(normal(0, .1), class = \"Intercept\", resp = \"wb\") +\n               prior(normal(0, .5), class = \"b\", resp = \"wb\") +\n               prior(exponential(1), class = \"sigma\", resp = \"wb\") +\n               prior(normal(0, .1), class = \"Intercept\", resp = \"support\") +\n               prior(normal(0, .5), class = \"b\", resp = \"support\") +\n               prior(exponential(1), class = \"sigma\", resp = \"support\")\n```\n:::\n\n\n## Mediation Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm2 <- brm(\n  formula = brm_mod_2,\n  data = d1,\n  family = gaussian(link = \"identity\"),\n  prior = brm_prior_2\n)\n```\n:::\n\n\n## Mediation Models\n\nLet's interpret this output together.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` nowrap-code\n Family: MV(gaussian, gaussian) \n  Links: mu = identity\n         mu = identity \nFormula: wb ~ support + soc_med \n         support ~ soc_med \n   Data: d1 (Number of observations: 3131) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nwb_Intercept          0.00      0.02    -0.03     0.03 1.00     5531     2936\nsupport_Intercept    -0.00      0.02    -0.03     0.03 1.00     6221     2969\nwb_support            0.47      0.02     0.43     0.50 1.00     5670     3091\nwb_soc_med            0.01      0.02    -0.02     0.04 1.00     5432     3086\nsupport_soc_med       0.21      0.02     0.18     0.25 1.00     5440     3053\n\nFurther Distributional Parameters:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_wb          0.88      0.01     0.86     0.91 1.00     6062     3233\nsigma_support     0.98      0.01     0.95     1.00 1.00     5444     2919\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Handling Categorical Predictors\n\nWhen predictors are categorical, we can use an index variable approach. This often involves treating the variables as nonlinear parameters to set distinct priors for each group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd2 <- d %>% select(sex, soc_med, support, wb)\n\nd2 <- d2 %>% filter(complete.cases(d2)) %>%\n    mutate(soc_med = (soc_med - mean(soc_med)) / sd(soc_med),\n         support = (support - mean(support)) / sd(support),\n         wb = (wb - mean(wb)) / sd(wb),\n         sex = factor(sex))\n\nsummary(d2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     sex          soc_med           support              wb         \n female:2279   Min.   :-2.7313   Min.   :-4.8823   Min.   :-2.6340  \n male  : 770   1st Qu.:-0.6349   1st Qu.:-0.5430   1st Qu.:-0.7372  \n other :  53   Median : 0.0639   Median : 0.1924   Median : 0.1480  \n               Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n               3rd Qu.: 0.6462   3rd Qu.: 0.7073   3rd Qu.: 0.7803  \n               Max.   : 2.3932   Max.   : 1.2956   Max.   : 1.9184  \n```\n\n\n:::\n:::\n\n\n## Handling Categorical Predictors\n\nNow, we want to estimate separate intercepts for each reported category of `sex`. \n\n* Make sure that `sex` is coded as a factor variable before fitting.\n\n* The code `0 + ` means to **not** estimate an overall intercept.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_mod_3 <- bf(wb ~ 0 + sex + support + soc_med) + \n             bf(support ~ 0 + sex + soc_med) + \n             set_rescor(FALSE)\n```\n:::\n\n\nNow, intercepts are indicated by `class = b` rather than `class = Intercept`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(brm_mod_3, data = d2, family = gaussian(link = \"identity\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` nowrap-code\n                prior class      coef group    resp dpar nlpar lb ub tag\n               (flat)     b                 support                     \n               (flat)     b sexfemale       support                     \n               (flat)     b   sexmale       support                     \n               (flat)     b  sexother       support                     \n               (flat)     b   soc_med       support                     \n student_t(3, 0, 2.5) sigma                 support             0       \n               (flat)     b                      wb                     \n               (flat)     b sexfemale            wb                     \n               (flat)     b   sexmale            wb                     \n               (flat)     b  sexother            wb                     \n               (flat)     b   soc_med            wb                     \n               (flat)     b   support            wb                     \n student_t(3, 0, 2.5) sigma                      wb             0       \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n```\n\n\n:::\n:::\n\n\n## Handling Categorical Predictors\n\nNow, we want a broader prior on the intercepts to allow for mean differences among the sexes. We'll use $N(0, .2)$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_prior_3 <- prior(normal(0, .2), class = \"b\", coef = \"sexfemale\", resp = \"wb\") +\n               prior(normal(0, .2), class = \"b\", coef = \"sexmale\", resp = \"wb\") +\n               prior(normal(0, .2), class = \"b\", coef = \"sexother\", resp = \"wb\") +\n               prior(normal(0, .5), class = \"b\", coef = \"soc_med\", resp = \"wb\") +\n               prior(normal(0, .5), class = \"b\", coef = \"support\", resp = \"wb\") +\n               prior(exponential(1), class = \"sigma\", resp = \"wb\") +\n               prior(normal(0, .2), class = \"b\", coef = \"sexfemale\", resp = \"support\") +\n               prior(normal(0, .2), class = \"b\", coef = \"sexmale\", resp = \"support\") +\n               prior(normal(0, .2), class = \"b\", coef = \"sexother\", resp = \"support\") +\n               prior(normal(0, .5), class = \"b\", coef = \"soc_med\", resp = \"support\") +\n               prior(exponential(1), class = \"sigma\", resp = \"support\")\n```\n:::\n\n\n\n## Handling Categorical Predictors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm3 <- brm(\n  formula = brm_mod_3,\n  data = d2,\n  family = gaussian(link = \"identity\"),\n  prior = brm_prior_3\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.54 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.14 seconds (Warm-up)\nChain 1:                0.126 seconds (Sampling)\nChain 1:                0.266 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.6e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.145 seconds (Warm-up)\nChain 2:                0.119 seconds (Sampling)\nChain 2:                0.264 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.6e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.14 seconds (Warm-up)\nChain 3:                0.124 seconds (Sampling)\nChain 3:                0.264 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.144 seconds (Warm-up)\nChain 4:                0.123 seconds (Sampling)\nChain 4:                0.267 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n## Handling Categorical Predictors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` nowrap-code\n Family: MV(gaussian, gaussian) \n  Links: mu = identity\n         mu = identity \nFormula: wb ~ 0 + sex + support + soc_med \n         support ~ 0 + sex + soc_med \n   Data: d2 (Number of observations: 3102) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nwb_sexfemale         -0.02      0.02    -0.06     0.02 1.00     7849     3213\nwb_sexmale            0.08      0.03     0.02     0.15 1.00     8538     2990\nwb_sexother          -0.29      0.10    -0.49    -0.08 1.00     7964     2940\nwb_support            0.46      0.02     0.43     0.50 1.00     7147     3010\nwb_soc_med            0.01      0.02    -0.02     0.05 1.00     6569     3267\nsupport_sexfemale     0.03      0.02    -0.01     0.07 1.00     7304     3137\nsupport_sexmale      -0.07      0.03    -0.13     0.00 1.00     8084     3265\nsupport_sexother     -0.28      0.12    -0.51    -0.06 1.00     8204     2891\nsupport_soc_med       0.21      0.02     0.17     0.24 1.00     7282     3368\n\nFurther Distributional Parameters:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_wb          0.88      0.01     0.86     0.90 1.00     7373     3061\nsigma_support     0.98      0.01     0.95     1.00 1.00     8819     3125\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Activity\n\nIn this lab, we fit 3 different `brms` models.\n\n* Multiple linear regression with 2 continuous predictors\n\n* Simple mediation with continuous predictor and mediator\n\n* Mediation with a categorical covariate\n\n## Activity\n\nTasks:\n\n1. Run the code to fit each model on your own computer.\n\n2. Visualize the results from each model using the code from this week and last. Feel free to explore other options.\n  \n  * `plot(conditional_effects(...))`: now use the fitted model rather than the priors-only model\n  \n      - Use both `method = \"posterior_linpred\"` and `method = \"posterior_predict\"`\n      \n      - Set `effect = \"...\"` to put a specific IV on the $x$ axis\n      \n      - Set `conditions = list(... = #)` to plot results conditional on a specific value of another predictor\n      \n      - Set `resp = \"...\"` to plot the effect for a specific outcome (response) ariable\n\n  * `pp_check(..., type = \"...\")`\n  \n      - Try `type = \"scatter_avg\", ndraws = 100`\n      \n3. For each plot, talk through with the TA and/or a classmate how to interpret the results.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}